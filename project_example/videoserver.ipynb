{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMEdOCDy5ItU1o+6AjHWURe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Esbern/Python-for-Planners/blob/main/project_example/videoserver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Reinforcement Learning and Echo Chambers in Social Media Through Video Recommendation Simulation\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "In the digital age, recommendation systems have become an essential part of our online experience. Social media platforms, video streaming services, and news aggregators rely on algorithms to suggest content based on user interactions. These algorithms often function as reinforcement learning systems, gradually adapting to user preferences. However, they also contribute to the formation of echo chambers, where users are increasingly exposed to content that aligns with their existing beliefs, limiting exposure to diverse perspectives.\n",
        "\n",
        "This document presents a Python simulation that models a simplified video recommendation system. The simulation mirrors how recommendation engines learn from user behavior and evolve to present more personalized content over time, illustrating the feedback loop that reinforces user preferences and potentially leads to echo chambers."
      ],
      "metadata": {
        "id": "uMwTe1zfS20y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "cxnlg5jsU2v_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "g1whSStcU7ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import random"
      ],
      "metadata": {
        "id": "XShhDgrHsa-0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulation Parameters"
      ],
      "metadata": {
        "id": "jqwSgaB6Uzfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "NUM_VIDEOS = 1000\n",
        "NUM_ITERATIONS = 1000\n",
        "NUM_RECOMMENDED_VIDEOS = 10\n",
        "LEARNING_RATE = 0.01  # How much the system updates per iteration\n",
        "SUBTRACTION_RATE = 1 # Keep sums of weights constant if 1 will together withe LEARNING_RATE = 0.01 increase echo chamber effer efect if increased\n",
        "\n",
        "# Topic List\n",
        "topics = [\n",
        "    \"Sports\", \"Video Games\", \"Politics\", \"Music\", \"Movies\", \"Science\", \"Technology\", \"History\", \"Travel\", \"Food\",\n",
        "    \"Health & Fitness\", \"Fashion\", \"Art\", \"Literature\", \"Business\", \"Finance\", \"Psychology\", \"Education\", \"Environment\", \"Space Exploration\",\n",
        "    \"Automobiles\", \"Home Improvement\", \"DIY & Crafts\", \"Gardening\", \"Cooking\", \"Pets\", \"Comedy\", \"Horror\", \"Mystery & Crime\", \"Anime & Manga\",\n",
        "    \"Fantasy & Sci-Fi\", \"Philosophy\", \"Religion\", \"Mythology\", \"Personal Development\", \"Relationships\", \"Parenting\", \"Photography\", \"Wildlife & Nature\", \"Extreme Sports\",\n",
        "    \"True Crime\", \"Architecture\", \"Interior Design\", \"Fitness & Bodybuilding\", \"Entrepreneurship\", \"Meditation & Mindfulness\", \"Productivity Hacks\", \"Social Media Trends\", \"Renewable Energy\", \"Artificial Intelligence\"\n",
        "]\n",
        "\n",
        "# Generate a list where each video is assigned a single topic\n",
        "video_topics = [random.choice(topics) for _ in range(NUM_VIDEOS)]\n",
        "\n",
        "# Define users with named topic preferences\n",
        "human_list = [\n",
        "    {\"name\": \"Alice\", \"user\": {topic: 0.5 for topic in topics}, \"preferences\": {\"Sports\": 0.8, \"Video Games\": 0.1, \"Music\": 0.9, \"Travel\": 0.6, \"Food\": 0.3}},\n",
        "    {\"name\": \"Bob\", \"user\": {topic: 0.5 for topic in topics}, \"preferences\": {\"Movies\": 0.7, \"Technology\": 0.4, \"History\": 0.9, \"Science\": 0.6, \"Finance\": 0.8}},\n",
        "    {\"name\": \"Charlie\", \"user\": {topic: 0.5 for topic in topics}, \"preferences\": {\"Health & Fitness\": 0.6, \"Art\": 0.9, \"Philosophy\": 0.5, \"Environment\": 0.7, \"Psychology\": 0.3}},\n",
        "    {\"name\": \"Diana\", \"user\": {topic: 0.5 for topic in topics}, \"preferences\": {\"Politics\": 0.4, \"Literature\": 0.8, \"Education\": 0.6, \"Architecture\": 0.9, \"Cooking\": 0.7}},\n",
        "    {\"name\": \"Eve\", \"user\": {topic: 0.5 for topic in topics}, \"preferences\": {\"Pets\": 0.9, \"Fashion\": 0.1, \"Anime & Manga\": 0.8, \"Mystery & Crime\": 0.6, \"Wildlife & Nature\": 0.7}},\n",
        "]\n",
        "# fill in missing values in the humans preferances\n",
        "for user in human_list:\n",
        "    preferences = {topic: 0.5 for topic in topics}  # Default neutral values\n",
        "    for topic, value in user[\"preferences\"].items():\n",
        "        preferences[topic] = value  # Corrected: Keep topic names, not indices\n",
        "\n",
        "    user[\"preferences\"] = preferences  # Update user preferences\n",
        "\n"
      ],
      "metadata": {
        "id": "0aCdvVNBsf3M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulation procedures"
      ],
      "metadata": {
        "id": "EzE41QC5VENo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_videos_to_present(user_id):\n",
        "    # Step 1: Select 10 topics randomly, weighted by preference values\n",
        "    user_preferences = human_list[user_id]['user']\n",
        "    selected_topics = random.choices(\n",
        "        list(user_preferences.keys()),  # List of topic names\n",
        "        weights=list(user_preferences.values()),  # Corresponding preference weights\n",
        "        k=10  # Select 10 topics\n",
        "    )\n",
        "\n",
        "    # Step 2: Select one random video for each of these topics\n",
        "    selected_videos = []\n",
        "    for topic in set(selected_topics):  # Use set to remove duplicates\n",
        "        topic_videos = [i for i, vid_topic in enumerate(video_topics) if vid_topic == topic]\n",
        "\n",
        "        if topic_videos:  # Ensure there are videos for the topic\n",
        "            selected_videos.append(random.choice(topic_videos))\n",
        "\n",
        "    return selected_videos\n",
        "\n",
        "\n",
        "def user_selects_the_best_video(user_id, recommended_videos):\n",
        "    video_preferences = []  # Store (video, preference_score) pairs\n",
        "\n",
        "    # Step 1: Compute preference score for each video\n",
        "    for vid in recommended_videos:\n",
        "        video_topic = video_topics[vid]  # Each video has one topic\n",
        "        preference_score = human_list[user_id]['preferences'].get(video_topic, 0.5)  # Default to 0.5 if missing\n",
        "        video_preferences.append((vid, preference_score))\n",
        "\n",
        "    # Step 2: Select a video randomly, weighted by its preference score\n",
        "    selected_video = random.choices(\n",
        "        [vid for vid, _ in video_preferences],  # Videos\n",
        "        weights=[score for _, score in video_preferences],  # Weights\n",
        "        k=1  # Select 1 video\n",
        "    )[0]\n",
        "\n",
        "    return selected_video\n",
        "\n",
        "def update_users_preferences(user_id, selected_video, recommended_videos):\n",
        "    selected_topic = video_topics[selected_video]  # The topic of the chosen video\n",
        "\n",
        "\n",
        "    # Step 1: Increase preference for the selected topic\n",
        "    human_list[user_id]['user'][selected_topic] += LEARNING_RATE\n",
        "\n",
        "    # Step 2: Reduce preference for rejected topics\n",
        "    num_rejected = len(recommended_videos) - 1  # Number of unchosen videos\n",
        "\n",
        "    if num_rejected > 0:  # Avoid division by zero\n",
        "        reduction_value = (LEARNING_RATE / num_rejected) * SUBTRACTION_RATE  # Keep total weight sum constant if SUBTRACTION_RATE = 1\n",
        "\n",
        "        for vid in recommended_videos:\n",
        "            rejected_topic = video_topics[vid]\n",
        "            if rejected_topic != selected_topic:  # Skip the chosen topic\n",
        "                human_list[user_id]['user'][rejected_topic] -= reduction_value\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zlm3e25UuIvw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run the simulation\n",
        "preference_history = {idx: [] for idx in range(len(human_list))}\n",
        "\n",
        "for iteration in range(NUM_ITERATIONS):\n",
        "    for user_id in range(len(human_list)):\n",
        "        recommended_videos = select_videos_to_present(user_id)\n",
        "        best_video = user_selects_the_best_video(user_id, recommended_videos)\n",
        "        update_users_preferences(user_id, best_video,recommended_videos)\n",
        "        preference_history[user_id].append(human_list[user_id]['user'].copy())\n",
        "\n",
        "\n",
        "\n",
        "# Define different line styles and markers\n",
        "line_styles = ['-', '--', '-.', ':']\n",
        "markers = ['o', 's', 'D', '^', 'v', 'p', '*', 'h', 'x', '+']\n",
        "style_cycle = itertools.cycle(line_styles)\n",
        "marker_cycle = itertools.cycle(markers)\n",
        "\n",
        "# Select the user to plot\n",
        "user_to_plot = 0\n",
        "print(human_list[user_to_plot][\"user\"])  # Print the final preferences for debugging\n",
        "\n",
        "# Create the figure\n",
        "plt.figure(figsize=(12, 8))  # Increased height to fit the legend below\n",
        "\n",
        "# Iterate through topics using the correct keys from user preferences:\n",
        "for topic_name in human_list[user_to_plot][\"preferences\"].keys():\n",
        "    values = [step.get(topic_name, 0.5) for step in preference_history[user_to_plot]]  # Use .get() to handle missing keys\n",
        "    plt.plot(values, label=topic_name, linestyle=next(style_cycle), marker=next(marker_cycle), markevery=100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "99QTbYYimki8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the result"
      ],
      "metadata": {
        "id": "fkA_3-ETZUQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels and Title\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Preference Score\")\n",
        "plt.title(f\"Evolution of {human_list[user_to_plot]['name']}'s Preferences Over Time\")\n",
        "\n",
        "# Adjust the legend to be below the plot\n",
        "plt.legend(\n",
        "    loc=\"upper center\",  # Places the legend below the plot\n",
        "    bbox_to_anchor=(0.5, -0.2),  # Moves the legend down\n",
        "    ncol=4,  # Organizes legend into 4 columns\n",
        "    fontsize='small',\n",
        "    frameon=False  # Removes legend box border for cleaner look\n",
        ")\n",
        "\n",
        "# Adjust layout to fit legend properly\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yf_79PSJVVME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Overview and Explanation\n",
        "\n",
        "The simulation is structured around a set of users, each with their own preferences for different topics. The recommendation system starts with no understanding of user preferences and refines its suggestions based on user selections.\n",
        "\n",
        "The simulation consists of three main processes:\n",
        "\n",
        "Selecting videos to present – The system picks 10 videos for each user based on their stored preferences.\n",
        "\n",
        "User selecting the best video – The user chooses a video based on their true preferences.\n",
        "\n",
        "Updating user preferences – The system updates its stored knowledge of the user's preferences based on their behavior.\n",
        "\n",
        "1. Selecting Videos to Present\n",
        "\n",
        "The function select_videos_to_present(user_id) simulates how a recommendation system selects videos by weighting topics based on the user's known preferences. Instead of picking completely random videos, the selection process is biased toward topics the user has previously engaged with.\n",
        "\n",
        "Code Example:\n",
        "\n",
        "import random\n",
        "\n",
        "def select_videos_to_present(user_id):\n",
        "    user_preferences = human_list[user_id]['user']\n",
        "    selected_topics = random.choices(\n",
        "        list(user_preferences.keys()),\n",
        "        weights=list(user_preferences.values()),\n",
        "        k=10\n",
        "    )\n",
        "    \n",
        "    selected_videos = []\n",
        "    for topic in set(selected_topics):\n",
        "        topic_videos = [i for i, vid_topic in enumerate(video_topics) if vid_topic == topic]\n",
        "        if topic_videos:\n",
        "            selected_videos.append(random.choice(topic_videos))\n",
        "    \n",
        "    return selected_videos\n",
        "\n",
        "This weighted random selection process mirrors real-world recommendation engines that prioritize content with higher engagement probabilities.\n",
        "\n",
        "2. User Selecting the Best Video\n",
        "\n",
        "Once videos are recommended, the user selects one based on their true preferences. Instead of always choosing the highest-rated video deterministically, the function introduces a degree of randomness to better reflect real-world user behavior.\n",
        "\n",
        "Code Example:\n",
        "\n",
        "def user_selects_the_best_video(user_id, recommended_videos):\n",
        "    video_preferences = []\n",
        "    for vid in recommended_videos:\n",
        "        video_topic = video_topics[vid]\n",
        "        preference_score = human_list[user_id]['preferences'].get(video_topic, 0.5)\n",
        "        video_preferences.append((vid, preference_score))\n",
        "\n",
        "    selected_video = random.choices(\n",
        "        [vid for vid, _ in video_preferences],\n",
        "        weights=[score for _, score in video_preferences],\n",
        "        k=1\n",
        "    )[0]\n",
        "    \n",
        "    return selected_video\n",
        "\n",
        "By making selections probabilistic, the simulation avoids overfitting to a single type of content, allowing for a more organic evolution of user preferences.\n",
        "\n",
        "3. Updating User Preferences\n",
        "\n",
        "Every time a user selects a video, the system updates its understanding of the user's preferences. The selected topic's preference score increases slightly, while the unchosen topics are slightly reduced. This reinforcement mechanism is a crucial element of how real-world platforms learn from user behavior.\n",
        "\n",
        "Code Example:\n",
        "\n",
        "def update_users_preferences(user_id, selected_video, recommended_videos):\n",
        "    selected_topic = video_topics[selected_video]\n",
        "    learning_rate = 0.01  \n",
        "\n",
        "    human_list[user_id]['user'][selected_topic] += learning_rate\n",
        "    num_rejected = len(recommended_videos) - 1  \n",
        "\n",
        "    if num_rejected > 0:  \n",
        "        reduction_value = learning_rate / num_rejected  \n",
        "        for vid in recommended_videos:\n",
        "            rejected_topic = video_topics[vid]\n",
        "            if rejected_topic != selected_topic:\n",
        "                human_list[user_id]['user'][rejected_topic] -= reduction_value\n",
        "\n",
        "This approach ensures that preferences dynamically adjust over time based purely on interaction history—without requiring direct knowledge of a user’s true preferences.\n",
        "\n",
        "Reinforcement Learning and Echo Chambers\n",
        "\n",
        "This simulation aligns closely with reinforcement learning (RL), where the algorithm learns through trial and error to maximize reward. In this case:\n",
        "\n",
        "The \"reward\" is the user’s video selection.\n",
        "\n",
        "The \"policy\" is the recommendation algorithm that adjusts based on past behavior.\n",
        "\n",
        "The risk of such a system is the emergence of echo chambers:\n",
        "\n",
        "If the user continues selecting the same type of content, the system will reinforce this preference and narrow down recommendations.\n",
        "\n",
        "Over time, the user is less likely to see diverse content, leading to an information bubble.\n",
        "\n",
        "This process is evident in social media platforms where engagement-driven algorithms often amplify divisive or sensational content because users repeatedly interact with it."
      ],
      "metadata": {
        "id": "i4KwRWOcUc3r"
      }
    }
  ]
}